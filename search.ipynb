{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc555124",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\katsf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\katsf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\katsf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\katsf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Βιβλιοθήκες\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from rank_bm25 import BM25Okapi\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405903eb",
   "metadata": {},
   "source": [
    "Ερώτημα 1\n",
    "a Επιλέξτε έναν ιστότοπο-στόχο ή ένα αποθετήριο ακαδημαϊκών εργασιών (π.χ. arXiv, PubMed ή αποθετήριο πανεπιστημίου). \n",
    "β. Υλοποιήστε έναν web crawler σε Python (π.χ. με BeautifulSoup) για τη συλλογή \n",
    "μεταδεδομένων ακαδημαϊκών εργασιών (τίτλος, συγγραφείς, περίληψη, ημερομηνία \n",
    "δημοσίευσης κ.λπ.) από την επιλεγμένη πηγή.\n",
    "γ. Αποθηκεύστε τα δεδομένα που συλλέγονται σε δομημένη μορφή, όπως JSON ή CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b972178d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def scrape_polynoe():\n",
    "    url = 'https://polynoe.lib.uniwa.gr/xmlui/browse?type=dateissued'\n",
    "    html = requests.get(url)\n",
    "    soup = BeautifulSoup(html.content, 'html.parser')\n",
    "    descriptions = soup.find_all('div', class_='artifact-description')\n",
    "    data = []\n",
    "\n",
    "    for desc in descriptions:\n",
    "        title = desc.find('h4', class_='artifact-title').text.strip()\n",
    "        author = desc.find('span', class_='author h4').text.strip()\n",
    "        date = desc.find('span', class_='date').text.strip()\n",
    "        abstract = desc.find('div', class_='artifact-abstract').text.strip()\n",
    "        data.append([title, author, date, abstract])\n",
    "\n",
    "    with open('data.json', 'w', encoding='utf8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20ca590",
   "metadata": {},
   "source": [
    "Ερώτημα 2\n",
    "Κάντε προεπεξεργασία του κειμενικού περιεχομένου των ακαδημαϊκών εργασιών για την προετοιμασία τους για ευρετηρίαση και αναζήτηση. \n",
    "Αυτό μπορεί να περιλαμβάνει εργασίες όπως tokenization, stemming/lemmatization και stop-word removal και αφαίρεση ειδικών χαρακτήρων \n",
    "(removing special characters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cced20b0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def preprocess_text():\n",
    "\n",
    "    stop_words = set(stopwords.words('greek'))  # Define stop_words\n",
    "    stemmer = PorterStemmer()  # Define stemmer\n",
    "    lemmatizer = WordNetLemmatizer()  # Define lemmatizer\n",
    "\n",
    "    with open('data.json', 'r', encoding='utf8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    processed_data = []\n",
    "    for entry in data:\n",
    "        title_tokens = word_tokenize(entry[0])\n",
    "        title_tokens = [word.lower() for word in title_tokens if word.isalpha() and word.lower() not in stop_words]\n",
    "        title_tokens = [stemmer.stem(word) for word in title_tokens]\n",
    "        title_tokens = [lemmatizer.lemmatize(word) for word in title_tokens]\n",
    "\n",
    "        author_tokens = word_tokenize(entry[1])\n",
    "        author_tokens = [word.lower() for word in author_tokens if word.isalpha() and word.lower() not in stop_words]\n",
    "        author_tokens = [stemmer.stem(word) for word in author_tokens]\n",
    "        author_tokens = [lemmatizer.lemmatize(word) for word in author_tokens]\n",
    "\n",
    "        abstract_tokens = word_tokenize(entry[3])\n",
    "        abstract_tokens = [word.lower() for word in abstract_tokens if word.isalpha() and word.lower() not in stop_words]\n",
    "        abstract_tokens = [stemmer.stem(word) for word in abstract_tokens]\n",
    "        abstract_tokens = [lemmatizer.lemmatize(word) for word in abstract_tokens]\n",
    "\n",
    "        processed_data.append({\n",
    "            'title': title_tokens,\n",
    "            'author': author_tokens,\n",
    "            'date': entry[2],\n",
    "            'abstract': abstract_tokens\n",
    "        })\n",
    "\n",
    "    with open('processed_data.json', 'w', encoding='utf8') as f:\n",
    "        json.dump(processed_data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24601e76",
   "metadata": {},
   "source": [
    "Ερώτημα 3\n",
    "α. Δημιουργήστε μια ανεστραμμένη δομή δεδομένων ευρετηρίου (inverted index) για την αποτελεσματική αντιστοίχιση όρων\n",
    " στα έγγραφα στα οποία εμφανίζονται. \n",
    "β. Εφαρμόστε μια δομή δεδομένων για την αποθήκευση του ευρετηρίου."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64f8fc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inverted_index():\n",
    "    with open('processed_data.json', 'r', encoding='utf8') as f:\n",
    "        data = json.load(f)\n",
    "    inverted_index = defaultdict(set)\n",
    "    for i, entry in enumerate(data):\n",
    "        for word in entry['abstract']:\n",
    "            inverted_index[word].add(i)\n",
    "    inverted_index = {k: list(v) for k, v in inverted_index.items()}\n",
    "    with open('inverted_index.json', 'w', encoding='utf8') as f:\n",
    "        json.dump(inverted_index, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da6e948",
   "metadata": {},
   "source": [
    "Ερώτημα 4\n",
    "α. Αναπτύξτε μια διεπαφή χρήστη για την αναζήτηση ακαδημαϊκών εργασιών χρησιμοποιώντας την Python (π.χ. μια διεπαφή γραμμής εντολών ή μια απλή διεπαφή ιστού). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c7ecf59",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def search(search_query):\n",
    "    print(\"Please choose an algorithm:\")\n",
    "    print(\"1. Boolean Retrieval\")\n",
    "    print(\"2. Vector Space Model\")\n",
    "    print(\"3. Okapi BM25\")\n",
    "    choice = int(input(\"Enter your choice (1-3): \"))\n",
    "\n",
    "    if choice == 1:\n",
    "        print(boolean_retrieval(search_query))\n",
    "    elif choice == 2:\n",
    "        print(ranking(search_query,'vectorspacemodel'))\n",
    "    elif choice == 3:\n",
    "        print(ranking(search_query,'okapibm25'))\n",
    "    else:\n",
    "        print(\"Invalid choice. Please enter a number between 1 and 3.\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6bc9a3",
   "metadata": {},
   "source": [
    "β. Υλοποιήστε πολλαπλούς (τουλάχιστον 3) αλγόριθμους ανάκτησης, όπως Boolean retrieval, \n",
    "Vector Space Model (VSM) και Probabilistic retrieval models (π.χ. Okapi BM25) για να \n",
    "ανακτήσετε σχετικές εργασίες με βάση τα ερωτήματα των χρηστών. Ο χρήστης θα μπορεί \n",
    "να επιλέγει τον αλγόριθμο ανάκτησης."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1067c583",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def boolean_retrieval(query):\n",
    "    query = query_processing(query)\n",
    "\n",
    "    # Load the inverted index from the JSON file\n",
    "    with open('inverted_index.json', 'r', encoding='utf8') as f:\n",
    "        inverted_index = json.load(f)\n",
    "\n",
    "    # Initialize the set of documents\n",
    "    docs = set(inverted_index.get(query[0], []))\n",
    "\n",
    "    # Apply Boolean operators\n",
    "    for i in range(1, len(query), 2):\n",
    "        operator = query[i]\n",
    "        word = query[i+1]\n",
    "\n",
    "        if operator.lower() == 'and':\n",
    "            docs &= set(inverted_index.get(word, []))\n",
    "        elif operator.lower() == 'or':\n",
    "            docs |= set(inverted_index.get(word, []))\n",
    "        elif operator.lower() == 'not':\n",
    "            docs -= set(inverted_index.get(word, []))\n",
    "\n",
    "    return list(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "787dd5f7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def vector_space_model(query):\n",
    "    # Load preprocessed documents from JSON file\n",
    "    with open('processed_data.json', 'r', encoding='utf8') as f:\n",
    "        documents = json.load(f)\n",
    "\n",
    "    # Tokenize the query\n",
    "    tokenized_query = word_tokenize(query.lower())\n",
    "\n",
    "    # Calculate TF-IDF\n",
    "    # Convert tokenized documents to text\n",
    "    preprocessed_documents = [' '.join(doc['title'] + doc['author'] + doc['abstract'] + [doc['date']]) for doc in documents]  # Combine all fields\n",
    "    preprocessed_query = ' '.join(tokenized_query)\n",
    "\n",
    "    # Create a TF-IDF vectorizer\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(preprocessed_documents)\n",
    "\n",
    "    # Transform the query into a TF-IDF vector\n",
    "    query_vector = tfidf_vectorizer.transform([preprocessed_query])\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    cosine_similarities = cosine_similarity(query_vector, tfidf_matrix)\n",
    "\n",
    "    # Rank documents by similarity\n",
    "    results = [(documents[i], cosine_similarities[0][i]) for i in range(len(documents))]\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Print the top 5 ranked documents\n",
    "    # for doc, similarity in results[:5]:  \n",
    "    #     print(f\"Similarity: {similarity:.2f}\\nTitle: {' '.join(doc['title'])}\\nAuthor: {' '.join(doc['author'])}\\nDate: {doc['date']}\\nAbstract: {' '.join(doc['abstract'])}\\n\")  # Print all fields\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e9b012d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def okapibm25(query):\n",
    "    # Load preprocessed documents from JSON file\n",
    "    with open('processed_data.json', 'r', encoding='utf8') as f:\n",
    "        documents = json.load(f)\n",
    "\n",
    "    # Tokenize the query\n",
    "    tokenized_query = query.split(\" \")\n",
    "\n",
    "    # Convert tokenized documents to text\n",
    "    preprocessed_documents = [' '.join(doc['title'] + doc['author'] + doc['abstract'] + [doc['date']]) for doc in documents]  # Combine all fields\n",
    "\n",
    "    # Initialize BM25Okapi model\n",
    "    bm25 = BM25Okapi([doc.split(\" \") for doc in preprocessed_documents])\n",
    "\n",
    "    # Get scores for each document\n",
    "    doc_scores = bm25.get_scores(tokenized_query)\n",
    "\n",
    "    # Get the indices of the top documents\n",
    "    top_indices = bm25.get_top_n(tokenized_query, range(len(preprocessed_documents)), n=5)\n",
    "\n",
    "    # # Print the details of the top documents\n",
    "    # for index in top_indices:\n",
    "    #     print(f\"Similarity Score: {doc_scores[index]}\")\n",
    "    #     print(f\"Title: {documents[index]['title']}\")\n",
    "    #     print(f\"Author: {documents[index]['author']}\")\n",
    "    #     print(f\"Abstract: {documents[index]['abstract']}\")\n",
    "    #     print(f\"Date: {documents[index]['date']}\")\n",
    "    #     print(\"\\n\")\n",
    "    results = [(documents[i], doc_scores[i]) for i in top_indices]\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dadc922",
   "metadata": {},
   "source": [
    "γ. Επιτρέψτε στους χρήστες να φιλτράρουν τα αποτελέσματα αναζήτησης με διάφορα \n",
    "κριτήρια, όπως η ημερομηνία δημοσίευσης ή ο συγγραφέας."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee97328b",
   "metadata": {},
   "source": [
    "def filter_results(criteria, value):\n",
    "    # Άνοιγμα του αρχείου με τα επεξεργασμένα δεδομένα\n",
    "    with open('processed_data.json', 'r', encoding='utf8') as f:\n",
    "        data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d0407a",
   "metadata": {},
   "source": [
    "    # Δημιουργία μιας λίστας με τα έγγραφα που πληρούν το κριτήριο\n",
    "    filtered_data = [doc for doc in data if doc.get(criteria) == value]\n",
    "        print(filtered_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a32c11",
   "metadata": {},
   "source": [
    "    # Επιστροφή της λίστας με τα φιλτραρισμένα δεδομένα\n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917489d4",
   "metadata": {},
   "source": [
    "Επεξεργασία ερωτήματος (Query Processing): Αναπτύξτε ένα module επεξεργασίας \n",
    "ερωτημάτων που θα προεπεξεργάζεται τα ερωτήματα που λαμβάνει από τον χρήστη, τα αναλύει \n",
    "και ανακτά σχετικά έγγραφα χρησιμοποιώντας το ανεστραμμένο ευρετήριο. Μπορείτε να \n",
    "χρησιμοποιήσετε απλά ερωτήματα βάσει λέξεων (όρων). Οι χρήστες θα πρέπει να μπορούν να \n",
    "αναζητούν έγγραφα χρησιμοποιώντας μία ή περισσότερες λέξεις. Το module θα λαμβάνει \n",
    "ερωτήματα χρηστών τα οποία τα γίνονται tokenized και θα εκτελεί λειτουργίες Boolean (AND, OR\n",
    "και NOT). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5effac25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_processing(query):\n",
    "\n",
    "    stop_words = set(stopwords.words('greek'))  # Define stop_words\n",
    "    stemmer = PorterStemmer()  # Define stemmer\n",
    "    lemmatizer = WordNetLemmatizer()  # Define lemmatizer\n",
    "\n",
    "    query_tokens = word_tokenize(query)\n",
    "    query_tokens = [word.lower() for word in query_tokens if word.isalpha() and word.lower() not in stop_words]\n",
    "    query_tokens = [stemmer.stem(word) for word in query_tokens]\n",
    "    query_tokens = [lemmatizer.lemmatize(word) for word in query_tokens]\n",
    "    \n",
    "    return query_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7a1629",
   "metadata": {},
   "source": [
    "Κατάταξη αποτελεσμάτων (Ranking): Εφαρμόστε έναν βασικό αλγόριθμο κατάταξης. Μπορείτε \n",
    "να ξεκινήσετε με έναν απλό αλγόριθμο κατάταξης TF-IDF (Term Frequency-Inverse Document\n",
    "Frequency) και αργότερα μπορείτε να συμπεριλάβετε πιο προηγμένες τεχνικές κατάταξης. \n",
    "Ταξινομήστε και παρουσιάστε τα αποτελέσματα αναζήτησης σε φιλική προς το χρήστη μορφή."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea757e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ranking(query, ranking_algorithm):\n",
    "    if ranking_algorithm == 'vectorspacemodel':\n",
    "        results = vector_space_model(query)\n",
    "    elif ranking_algorithm == 'okapibm25':\n",
    "        results = okapibm25(query)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported ranking algorithm\")\n",
    "    for doc, similarity in results:\n",
    "        print(f\"Similarity: {similarity:.2f}\\nTitle: {' '.join(doc['title'])}\\nAuthor: {' '.join(doc['author'])}\\nDate: {doc['date']}\\nAbstract: {' '.join(doc['abstract'])}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c5a1f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3b82d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your search query:  ορθή\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please choose an algorithm:\n",
      "1. Boolean Retrieval\n",
      "2. Vector Space Model\n",
      "3. Okapi BM25\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your choice (1-3):  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    scrape_polynoe()\n",
    "    preprocess_text()\n",
    "    create_inverted_index()\n",
    "    search_query = input(\"Enter your search query: \")\n",
    "    #filters = input(\"Enter your filter: \")\n",
    "    search(search_query)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,py:light",
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
